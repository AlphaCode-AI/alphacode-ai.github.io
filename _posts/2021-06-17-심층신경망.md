---
layout: post
title: 심층 신경망
date: 2021-06-16
author: jinah
tags: [Study]
---

## 단층 신경망 	<sub>Single-Layer Perceptron</sub>
---

- input layer와 outoput layer 로만 구성
- 선형으로 이루어짐

<img src="https://wikidocs.net/images/page/24958/perceptron3_final.PNG">




  
### **단층 신경망의 한계**
---
- 선형 분리 불가능(linearly inseparable) 문제

> - 직선으로 영역을 분리할 수 없는 경우가 있음
>  ![선형 분리 불가능](/assets/img/img_0302/LinearlyInseparable.png)  

> - 이러한 경우 아래 그림과 같이 복잡한 곡선으로만 영역 분리 가능함 
> - 선형분류기에서 비선형 분류기로 비꿔야하는 필요성 있음
> - 이를 해결하기 위해 나온 개념이 **hidden layer**
> 
> <img src="https://wikidocs.net/images/page/24958/xorgate_nonlinearity.PNG">



<img src="https://heung-bae-lee.github.io/image/shallowNN_01.png">





<img src="https://heung-bae-lee.github.io/image/Deep_Neural_Network.png">

## 심층신경망 특징
---
- hidden layer가 2개 이상 부터는 심층신경망
-  비선형의 활성함수


> - hidden layer도 무작정 쌓기만 한다고 해서 퍼셉트론을 선형분류기에서 비선형분류기로 바꿀 수 있는 것은 아님  
> -  2개의 Layer를 쌓아봤지만 X에 곱해지는 항들은 W로 치환가능하고, 입력과 무관한 상수들은 전체를 B로 치환 가능하기 때문에 WX+B라는 Single layer perceptron과 동일한 결과   
> - Deep 하게 쌓는 의미가 없어짐
> 
><img src="https://t1.daumcdn.net/cfile/tistory/993FAA4B5B6CF9BC03">


## 활성화 함수<sub>activation function<sub/>
---
![neuron](/assets/img/img_0302/neuron.png)  
<img src="https://databuzz-team.github.io/images/danial/back-prop/network_detail.png">


### 활성화 함수의 종류
---

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=http%3A%2F%2Fcfile21.uf.tistory.com%2Fimage%2F991410475B99D9D814AF34">


### 한계
--- 
- non-linear 문제들은 해결할 수 있었지만 layer가 깊어질수록 파라미터의 개수가 급등하게 되고 이 파라미터들을 적절하게 학습시키는 것이 매우 어려움
- 이는 **역전파 알고리즘**이 등장하게 되면서 해결되었고 결론적으로 여러 layer를 쌓은 신경망 모델 학습이 가능




## 역전파<sub> Backpropagation <sub/>
- 역전파 알고리즘은 출력값에 대한 입력값의 기울기(미분값)을 출력층 layer에서부터 계산하여 거꾸로 전파시키는 것
-  출력층 바로 전 layer에서부터 기울기(미분값)을 계산하고 이를 점점 거꾸로 전파시키면서 전 layer들에서의 기울기와 서로 곱하는 형식으로 나아가면 최종적으로 출력층의 output에 대한 입력층에서의 input의 기울기(미분값)을 구할 수가 있음

<img src="https://heung-bae-lee.github.io/image/Back_Propagation.png">


<img src="https://cdn.datamaker.io/django-rdproject/uploads/2020/09/16/image-1_uzw2Se8.png">


### 배치<sub> Batch <sub/>
- Iteration 1회당 사용되는 training data set 의 묶음

### 미니배치sub>Mini-Batch <sub/>
-  training data set 쪼개어 놓은 묶음
<img src="https://mblogthumb-phinf.pstatic.net/MjAxOTEwMzFfMjIg/MDAxNTcyNTE0MTM3NTA3.NnsTR9RUZV-PziKHTGaq7Uy4hpd8Ccj1SQLjwWABeVgg.JptxkYdQbthGxwSLQVVpBRKmghG9ufcvqXf7Y42wtvwg.PNG.myincizor/%EA%B7%B8%EB%A6%BC1.png?type=w800">

###  Batch <sub> gradient descent(BGD) <sub/>
- 전체 데이터 셋에 대한 에러를 구한 뒤 기울기를 한번만 계산하여 모델의 parameter 를 업데이트 하는 방법

###  확률적 경사 하강법 <sub> Stochastic gradient descent(SGD)

- 추출된 데이터 한 개에 대해서 error gradient 를 계산하고, Gradient descent 알고리즘을 적용하는 방법
- 전체 데이터를 사용하는 것이 아니라, 랜덤하게 추출한 일부 데이터를 사용
- 따라서 학습 중간 과정에서 결과의 진폭이 크고 불안정하며, 속도가 매우 빠르다. 

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fb8RUdK%2FbtqAm4Utey2%2F5DfjTQw70XKYgtOB8VHpeK%2Fimg.png">



