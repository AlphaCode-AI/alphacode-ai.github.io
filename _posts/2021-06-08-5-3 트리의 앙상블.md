---
layout: post
title: íŠ¸ë¦¬ì˜ ì•™ìƒë¸”
published: true
date:   2021-06-08 00:00:00 +0900
feature-img: "assets/img/feature-img/totoro.png"
thumbnail: "assets/img/thumbnails/feature-img/totoro.png"
excerpt:
author: inseon
tags: [Test, inseonchoi, Markdown]
---

> **3-3 Ensemble Of Tree** 

### 1. Ensemble(ì•™ìƒë¸”) 

<img src="https://upload.wikimedia.org/wikipedia/commons/5/59/KneiselQuartet.jpg" width="700"/>

- **An ensemble is a group of people who work or perform together.**
- ì•™ìƒë¸”(ensemble)ì€ ì „ì²´ì ì¸ ì–´ìš¸ë¦¼ì´ë‚˜ í†µì¼. 'ì¡°í™”'ë¡œ ìˆœí™”í•œë‹¤ëŠ” ì˜ë¯¸ì˜ í”„ë‘ìŠ¤ì–´ 
- ìŒì•…ì—ì„œ 2ì¸ ì´ìƒì´ í•˜ëŠ” ë…¸ë˜ë‚˜ ì—°ì£¼ë¥¼ ë§í•˜ë©°, 
- ë®¤ì§€ì»¬ì—ì„œ ì£¼, ì¡°ì—° ë°°ìš°ë“¤ ë’¤ì—ì„œ í™”ìŒì„ ë„£ê³  ì¶¤ì„ ì¶”ê³  ë…¸ë˜ë¥¼ ë¶€ë¥´ë©´ì„œ ë¶„ìœ„ê¸°ë¥¼ ë‹êµ¬ëŠ” ì—­í•  


ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€
ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€
ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€
ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€


ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€
ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€
ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€
ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€


### 2. Treeì˜ Ensemble 

<img src="https://miro.medium.com/max/3200/1*-XBxuOgB5j0irQiB9dRubA.jpeg" width="1400"/>

- Decision Tree Algorithm
- **Ensemble** methods are meta-algorithms that conbine several machine learning techniques into one predictive model 
- in order to **decrease variance**(bagging) and **bias**(boosting)

#### Ensemble methods can be divided into two groups
- Parallel ensemble methods where the base learners are generated parallel (e.g Random Forest)
    - exploit **independence** between the base learners 
    - since the error can be reduced dramatically by averaging 
- Sequential ensemble methods where the base learners are generated sequentially (e.g AdaBoost)
    - exploit the **dependence** between the base learners 
    - The overall performance can be boosted by weighing previously mislabeld examples with higher weight

<img src="https://miro.medium.com/max/2000/1*zTgGBTQIMlASWm5QuS2UpA.jpeg" width="1400"/>


ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€
ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€
ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€
ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€


ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€
ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€
ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€
ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€


### 3. Bagging (â†’Decrease Variance)
- Bootstrap ë°©ì‹ì˜ ì•™ìƒë¸” í•™ìŠµë²•
- Bootstrap aggregating 
- Baggingì€ ë¶„ì‚°ì„ ì¤„ì´ê³  overfittingì„ í”¼í•˜ë„ë¡ í•´ì¤Œ
- Decision Treeë‚˜ Random Forestì— ì ìš©ë˜ëŠ” ê²ƒì´ ì¼ë°˜ì 


#### Bootstrap 

<img src="https://www.researchgate.net/profile/Paola-Galdi/publication/322179244/figure/fig2/AS:588191077777408@1517247089079/An-example-of-bootstrap-sampling-Since-objects-are-subsampled-with-replacement-some.png" width="1400"/>

- ì²« ë²ˆì§¸ ì°¨ë¡€ì—ì„œ íŒŒë€ ê³µì´ ë½‘í í™•ë¥  = 1/3
- ë‘ ë²ˆì§¸ ì°¨ë¡€ì—ì„œ íŒŒë€ ê³µì´ ë½‘í í™•ë¥  = 1/3
- ì„¸ ë²ˆì§¸ ì°¨ë¡€ì—ì„œ íŒŒë€ ê³µì´ ë½‘í í™•ë¥  = 1/3 
- 9ê°œ ê³µì´ ëª¨ë‘ íŒŒë€ìƒ‰ì¼ ìˆ˜ ìˆë‹¤ 


#### ê°œë³„ ë¶„ë¥˜ê¸°ë³´ë‹¤ ì„±ëŠ¥ì´ ë›°ì–´ë‚œ ì´ìœ  
- ì´í•­ë¶„í¬ì˜ í™•ë¥  ì§ˆëŸ‰ í•¨ìˆ˜ 
- ë‹¨ì¼ Treeì˜ Binary Class í™•ë¥ ì— ëŒ€í•´ Nê°œì˜ Treeë¡œ í™•ì¥ ì‹œì¼°ì„ ë•Œì˜ ì˜¤ì°¨ìœ¨ 
<img src="/assets/img/pexels/B(N,P).png" width="1400"/>
- ë”°ë¼ì„œ ì•™ìƒë¸”ì˜ ì—ëŸ¬ í™•ë¥ ì€ ê°œë³„ ë¶„ë¥˜ê¸° ë³´ë‹¤ í•­ìƒ ì¢‹ë‹¤.
- ë‹¤ë§Œ ê°œë³„ ë¶„ë¥˜ê¸°ì˜ ì—ëŸ¬ìœ¨ì´ 0.5ì´ë©´ì„œ ì•™ìƒë¸”í•  ë¶„ë¥˜ê¸°ê°€ ì§ìˆ˜ ì¼ ë•Œ, ì˜ˆì¸¡ì´ ë°˜ë°˜ìœ¼ë¡œ ë‚˜ë‰˜ë©´ ì—ëŸ¬ë¡œ ì·¨ê¸‰ëœë‹¤. 
- ë”°ë¼ì„œ ê°œë³„ ë¶„ë¥˜ê¸°ê°€ ë¬´ì‘ìœ„ ì¶”ì¸¡(P(E)=0.5)ë³´ë‹¤ëŠ” ì„±ëŠ¥ì´ ì¢‹ì•„ì•¼ í•œë‹¤. 

#### A Commonly used class of ensemble algorithms are **forest of randomized trees**
- In random forests, each tree in the ensemble is built from a sample drawn with replacement (i.e a bootstrap sample) from the traning set. 
- In addition, instead of using all the features, a random subset of features is selected, further randomizing the tree 
- As a result, the bias of the forest increases slightly, but due to the averaging of less correlated trees (ìƒê´€ê´€ê³„ê°€ ì ì€ treeì˜ í‰ê· í™” ë•Œë¬¸ì—), its **variance decreases**, resulting in an overall better 
- ê° Treeì˜ Predictionì„ ëª¨ì•„ ë‹¤ìˆ˜ê²° íˆ¬í‘œ(Majority voting)ë¡œ ì˜ˆì¸¡ 





ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€
ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€
ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€
ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€


ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€
ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€
ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€
ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€


### 4. Boosting (â†’Decrease Bias)
<img src="https://cdn.mos.cms.futurecdn.net/hcCDvyCrqGzs58bhye33LB.jpg" width="1400"/>

#### Boosting
- the hypothesis boosting
- Reduce Bias 
- Improve Performance

#### Compare with Bagging
- ê³µí†µì  
    - 1ê°œì˜ Strong Learnerê°€ ì•„ë‹Œ nê°œì˜ Weak Learner Modelì˜ Ensemble ë°©ì‹ 
    - Weak Learnerë¥¼ ë‹¤ìˆ˜ê²° íˆ¬í‘œ(Majority voting)ë¡œ ì—°ê²°
- ì°¨ì´ì 
    - ê° ëª¨ë¸ì„ ë§Œë“¤ê¸° ìœ„í•œ samplingì—ì„œ ì¤‘ë³µì„ í—ˆìš©í•˜ì§€ ì•ŠìŒ (Bootstrap ë°©ì‹ X)
    - ìƒí˜¸ ì˜ì¡´ì ì¸ ê°œë³„ ëª¨ë¸
    - ì´ì „ Modelì˜ ì˜í–¥ì„ ë°›ì•„ì„œ Errorì— ë” í° ê°€ì¤‘ì¹˜ ë¶€ì—¬ 



<img src="https://www.codingninjas.com/blog/wp-content/uploads/2020/08/3-1.png" width="1400"/>




#### AdaBoost
<img src="https://miro.medium.com/max/544/1*m2UHkzWWJ0kfQyL5tBFNsQ.png" width="1400"/>

#### Gradienc Boosting 
- AdaBoostì™€ Gradient Boostingì€ ì „ë°˜ì ì¸ ê°œë…ì´ ê°™ë‹¤.
- Weak (ê¹Šì´ê°€ 1ì¸ Decision Treeì™€ ê°™ì€) Learnerë¥¼ Boostingí•˜ì—¬ Strong Modelì„ ë§Œë“ ë‹¤.

<img src="https://static.packt-cdn.com/products/9781788295758/graphics/B07777_04_table1-1.jpg" width="1400"/>

#### XGBoost


<img src="https://blog.kakaocdn.net/dn/rwwQd/btqXBWOCMUy/ExD4Oma2sldiF7HHZlRkH0/img.jpg" width="1400"/>

#### LightGBM

<img src="https://miro.medium.com/max/3076/1*uxyv1tqrFiKyJ3mYsuybSw.png" width="1400"/>

<img src="https://miro.medium.com/max/3400/1*A0b_ahXOrrijazzJengwYw.png" width="1400"/>

<img src="https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRNG-HDeLSg9uLbyt1sjkBNbWZ0lfWIKjhAZ1ogdm7Z38nipxEwWCYRhyxTYgoz-3F3aVs&usqp=CAU" width="800"/>

ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€
ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€
ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€
ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€


ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€
ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€
ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€
ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€


### 5. Quiz
<img src="https://cdn.analyticsvidhya.com/wp-content/uploads/2020/08/eba93f5a75070f0fbb9d86bec8a009e9.png" width="800"/>
1. Varianceë¥¼ ë‚®ì¶”ê¸° ìœ„í•œ Ensemble ë°©ì‹ì€? ğŸ¤¡

(a) Bagging ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€   (b) Gradienc Descent   
(c) Boostingã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€   (d) Hyperparameter Tuning








