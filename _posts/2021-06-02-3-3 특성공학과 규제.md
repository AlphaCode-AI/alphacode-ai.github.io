---
layout: post
title: 3-3. 특성공학과 규제
published: true
date:   2021-06-02 17:27:00 +0900
feature-img: "assets/img/feature-img/totoro.png"
thumbnail: "assets/img/thumbnails/feature-img/totoro.png"
excerpt:
author: inseon
tags: [Test, inseonchoi, Markdown]
---

> 3-3 Feature Engineering과 Regularization

**혼공 머신 학습 목표**
- 여러 특성(**Feature**)를 사용한 다중 회귀(**Multiple Regression**)에 대해 배우고, 사이킷런(Scikt-learn)의 여러 도구를 사용해 본다. 
- 복잡한 모델의 과대적합(**Overfitting**)을 막기 위한 릿지(**Ridge**)와 라쏘(**LASSO**) 회귀를 배운다. 

**핵심 키워드** 
- Multiple Regression, Feature Engineering, Ridge, LASSO, Hyperparameter 
　　　　　　　　　　
　　　　　　　　　　
　　　　　　　　　　
　　　　　　　　　　


　　　　　　　　　　
　　　　　　　　　　
　　　　　　　　　　
　　　　　　　　　　


　　　　　　　　　　
　　　　　　　　　　
　　　　　　　　　　
　　　　　　　　　　

### 1. Multiple Regression Model 
- **1-1. Regression**
    - '회귀(**Regression**)'라는 단어가 일상에서 잘 안쓰이고 직관적이지 못함
    - **회귀**의 사전적 의미는 **'한바퀴 돌아 제자리로 돌아가다'**
    - 통계학에서 회귀란, **여러 개의 독립 변수(Feature)**와 **한 개의 종속 변수(Target)** 간의 **상관관계**를 모델링 하는 기법을 통칭한다.
    - 수학적으로는 독립 변수 X로 종속 변수 Y를 표현하는 추세선 F'(X)를 찾아가는 과정 
    - 데이터와 오차합이 가장 작은 함수(Model)을 찾는 작업

<img src="https://img1.daumcdn.net/thumb/R720x0.q80/?scode=mtistory2&fname=http%3A%2F%2Fcfile7.uf.tistory.com%2Fimage%2F997E924F5CDBC1A6283C93" width="500"/>


　　　　　　　　　　
　　　　　　　　　　
　　　　　　　　　　
　　　　　　　　　　
　　　　　　　　　　
　　　　　　　　　　
　　　　　　　　　　
　　　　　　　　　　

- **1-2. Regression 분류**
    - Simple (Linear) Regression Model
    - Multiple Regression

<img src="/assets/img/pexels/regression.png" width="1000"/>

　　　　　　　　　　
　　　　　　　　　　
　　　　　　　　　　
　　　　　　　　　　
- **1-3. Multiple Regression Model**
    - 사용 가능한 Feature가 늘어날 수록 모델의 설명력이 올라감 (=성능이 좋다)
    - 그러나 현실에서 사용 가능한 데이터(Feature)는 제한적
　　　　　
　　　　　

>**Multiple Regression Model의 성능을 높이기 위해 Feature를 생성하는 방법은?**
- Feature Engineering

　　　　　　　　　　
　　　　　　　　　　
　　　　　　　　　　
　　　　　　　　　　

　　　　　　　　　　
　　　　　　　　　　
　　　　　　　　　　
　　　　　　　　　　


　　　　　　　　　　
　　　　　　　　　　
　　　　　　　　　　
　　　　　　　　　　

### 2. Feature Engineering 
- 주어진 Feature를 조합하여 새로운 Feature를 만드는 일련의 작업 과정
    - STX엔진에서 새로운 Column 추가했던 작업 (Moving Average 값, **값 등)


**2-1. 고차항을 통한 Feature Engineering**
- 가정 
    - 주어진 데이터에 X1, X2 두개의 Feature 가 존재
    - record 1개 
    - Raw Data = [[X1, X2]]
　　　　　　　　　　
　　　　　　　　　　
　　　　　　　　　　
　　　　　　　　　　
- 1) Feature를 **제곱(^2)**하여 새로운 Feature 생성
    - X1^2, X2^2, X1X2, X1, X2
    - 사용 가능한 Feature 개수가 2에서 5로 증가 
　　　　　　　　　　
　　　　　　　　　　
　　　　　　　　　　
　　　　　　　　　　
- 2) Feature를 **세제곱(^3)**하여 새로운 Feature 생성
    - X1^3, X2^3, X1^2*X2,  X1 *X2^2, X1^2, X2^2, X1X2, X1, X2
    - 사용 가능한 Feature 개수가 2에서 9로 증가 

　　　　　　　　　　
　　　　　　　　　　
　　　　　　　　　　
　　　　　　　　　　

>### Sckit-learn Example 
```python
from sklearn.preprocessing import PolynomialFeatures
X1 = 2 
X2 = 3
data = [X1, X2]
```

>```python
poly = PolynomialFeatures()   # PolynomialFeatures(include_bias=False, degree=5)
poly.fit([data])
print(poly.transform([data]))
```
```python
=> [[1. 2. 3. 4. 6. 9.]]
```
*** 최대 5제곱까지 가능**
```python
poly = PolynomialFeatures(include_bias=False, degree=5)
poly.fit([data])
print(poly.transform([data]))
```
```python
=> [[  2.   3.   4.   6.   9.   8.  12.  18.  27.  16.  24.  36.  54.  81.
   32.  48.  72. 108. 162. 243.]]
```

　　　　　　　　　　
　　　　　　　　　　
　　　　　　　　　　
　　　　　　　　　　

**2-2. Feature는 많을수록 좋은건가요?**
- Feature가 많을 수록 모델이 복잡해 진다.
- 모델이 복잡해질 수록 Train Set에 대한 성능은 정확도 100에 가까워집니다.


  <img src="https://blog.kakaocdn.net/dn/Td9EY/btqxJx5IVHD/5gVSXVPgzNG85E49PLvYw1/img.png" width="1000"/>

- 다시 말해, 오차가 거의 존재하지 않으며, 이러한 상황은 Test set에 적용하지 못하는 OverFitting 문제가 발생 
 <img src="https://miro.medium.com/max/973/1*nhmPdWSGh3ziatQKOmVq0Q.png" width="700"/>
- **OverFitting** 문제
    - 일반화 어려움
    - 높은 분산

　　　　　　　　　　
　　　　　　　　　　
　　　　　　　　　　
　　　　　　　　　　

**2-3. Bias와 Variance**

<img src="https://user-images.githubusercontent.com/31475037/59551500-8bded580-8fb5-11e9-8bff-c06a777645e4.png" width="1000"/>

-Bias-Variance **Tradeoff** 

<img src="https://cdn.analyticsvidhya.com/wp-content/uploads/2020/08/eba93f5a75070f0fbb9d86bec8a009e9.png" width="700"/>


　　　　　　　　　　
　　　　　　　　　　
　　　　　　　　　　
　　　　　　　　　　

>**적절한 Tradeoff 지점을 찾는 방법**
- 오차(Training Error)항의 **Regularization**(규제) 

　　　　　　　　　　
　　　　　　　　　　
　　　　　　　　　　
　　　　　　　　　　


　　　　　　　　　　
　　　　　　　　　　
　　　　　　　　　　
　　　　　　　　　　

### 3.Regularization 
**3-1. 복습**
- **Regressiong** 
    - 데이터를 표현하는 최적의 회귀선을 찾는 것으로 각 독립변수(**Feature**)의 최적의 가중치(**Weight**)를 찾는 과정 
    - 오차를 최소화 하는 **Weigh**값의 반복적인 **Update**
    - Weight(계수)는 기울기에 해당하는데,... 
    - 기울기가 가파를 수록 Overfitting 
- **Machine Learning**
    - 빠르고 복잡한 연산이 가능한 컴퓨터가 인간 대신 최적의 회귀선을 찾는 것


- **Error(=Cost)** 
    - 오차 = 예측 값-실제 값 = F'(x) - F(x)
    - F`(x) = 모델을 이용한 예측 값 = w1x1^2 + w2x2^2 + w3...
    - Feature (x1, x2, .. xn)가 많을 수록, 복잡한 모델이 만들어지며 Train Set에 대한 오차가 줄어듬
    - train set에 대해 거의 완벽한 성능을 가지지만, 복잡하기 때문에 기울기가 가파를 수있다

- **OverFitting**
    - 복잡한 모델로 인해 Train Set에 대해 완벽한 성능을 가졌으나 Test set에 대한 성능이 떨어지는 것
    - 높은 분산과 일반화 어려움


　　　　　　　　　　
　　　　　　　　　　
　　　　　　　　　　
　　　　　　　　　　
　　　　　　　　　　
　　　　　　　　　　
　　　　　　　　　　

**3-2. Regularization**
- 복잡한 모델의 OverFitting 문제 해결을 위함
- **기울기가 가파른 것을 제거하기 위해서** 오차를 인위적으로 **크게** 만드는 것 
    - 오차 = 실제값-예측값 **+규제항**
    - 규제항 = **Wiehgt**(가중치,계수,기울기)의 합 
    - Weigth가 커질 수록 오차가 커짐
    - 다시 말해 Wieght가 커지는 것에 제한(=규제)을 둠

　　　　　　　　　　
　　　　　　　　　　
　　　　　　　　　　
　　　　　　　　　　

**3-3. Regularization 분류**
- 규제 종류에 따라 Linear Regression Model은 **Ridge**(L2)와 **LASSO**(L1)로 분류 된다
- 오차를 계산하는 방법에 따라 RMSE, MAE등 다양한 오차 함수가 존재
    - RMSE : 오차 제곱 합의 평균
    - MAE : 오차 절대 값 합의 평균
- 규제항도 계산법에 따라 L2, L1 등으로 나뉘어짐 
    - **L2** : 가중치 제곱의 합
    - **L1** : 가중치 절대값의 합 
- 규제 종류에 따라 Linear Regression Model은 **Ridge**(L2)와 **LASSO**(L1)로 분류 된다
 <img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbxmigS%2FbtqxIE5KCFE%2FjkIsNfmB3MBKWdjl8KqWY0%2Fimg.png" width="700"/>
    - **Ridge** : 좋은 성능을 보여 주로 사용하는 규제 방법 중 하나
    - **LASSO** : 수학적으로 가중치 0이 가능하여, 불필요한 Feature를 가려내는 용도로 사용 가능
　　　　　　　　　　
　　　　　　　　　　
　　　　　　　　　　
　　　　　　　　　　

- (참고)Deep learning 규제 방법 중 하나인 Drop Out
　　　　　　　　　　
　　　　　　　　　　
　　　　　　　　　　

<img src="https://miro.medium.com/max/1044/1*iWQzxhVlvadk6VAJjsgXgg.png" width="700"/>
　　　　　　　　　　
　　　　　　　　　　
　　　　　　　　　　
　　　　　　　　　　

　　　　　　　　　　
　　　　　　　　　　
　　　　　　　　　　
　　　　　　　　　　

### 4. Hyperparameter 
- 머신러닝이란, 컴퓨터가 주어진 데이터를 설명할 수 있는 모델을 학습하는데,
- 인간보다 연산기능이 좋은 컴퓨터가 복잡한 연산을 통해 Feature별 가장 최적의 가중치를 찾는것이다.
- 하지만 모델을 학습하기 위해서 사람이 직접 지정하여 최적의 값을 찾아내야하는 변수가 있는데, 이를 하이퍼파라미터라 한다.
- 다들 잘 알고 있는 Learning Rate, Train Node Hour가 이에 속한다. 
- **규제항을 추가할 때도, 규제 정도를 사람이 지정할 수 있다.**
- 규제 정도에 따라 규제 성능이 다르다. (반복 실험을 통해 찾아야 한다.)


　　　　　　　　　　
　　　　　　　　　　
　　　　　　　　　　
　　　　　　　　　　
### 😇😎🤓

