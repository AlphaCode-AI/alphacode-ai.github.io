---
layout: post
title: LSTM과 GRU 셀
date: 2021-06-28
author: naran
tags: [Machine learning, Deep learning, Study, LSTM, GRU]
---

### Basic RNN 알고리즘의 약점

> The cat, which already ate ...., was full <br>
> The cats, which already ate ...., were full


* 문장 초반부의 cat/cats에 따라 문장 후반부의 was/were가 달라지는 것을 알 수 있음
* 이는 문장 초반부의 단어가 문장 후반부에 영향을 끼친다는 것을 의미
* RNN은 기본적으로 많은 토큰들을 다루기 때문에 깊은 신경망으로 이루어져있으며
* Back Propagation을 사용해 가중치를 업데이트하기 때문에 
* 문장 초반부의 단어가 결과에 미치는 영향은 문장 후반부의 단어가 결과에 미치는 영향으로부터 계산된다
* 신경망이 깊어질수록 문장 초반부의 단어가 결과에 미치는 영향이 적어져 
* 가까운 곳에 있는 입력에 영향을 더 많이 받을 수 밖에 없다
* 이것이 Basic RNN 알고리즘의 약점이다


### LSTM 
- Long Short-Term Memory 의 약자
- 타임스텝이 긴 데이터를 효과적으로 학습하기 위해 고안된 순환층
- 단기 기억을 오래 기억하기 위해 고안됨
- 입력과 가중치를 곱하고 절편을 더해 활성화 함수를 통과시키는 구조를 여러 개 가지고 있음 (4개)
- 입력 게이트, 삭제 게이트, 출력 게이트 역할을 하는 작은 셀이 포함되어 있음
- 은닉 상태 외에 셀 상태를 출력하며 셀 상태는 다음 충으로 전달되지 않으며 현재 셀에서만 순환됨


![](/assets/img/feature-img/lstm.png)
![](/assets/img/feature-img/lstm-layer.png)

### 셀 상태<sup>Cell state</sup>
![](/assets/img/feature-img/lstm-cell-state.png)
- 메모리와 같은 존재이며 은닉 상태와 달리 다음 층으로 전달되지 않고 셀에서 순환만 되는 값
- 컨베이어 벨트와 같아서 State가 꽤 오래 경과하더라도 Gradient가 잘 전파됨
- Gate라 불리는 구조에 의해 정보가 추가되거나 제거되며 <br>Gate는 Training을 통해 어떤 정보를 유지하고 버릴지 학습하며 <br>Cell State를 보호하고 제어하는 역할을 함
	- Forget gate (f) : 과거 정보를 잊기 위한 게이트
	- Inout gate (i) : 현재 정보를 기어하기 위한 게이트
	- Output gate (o) : 최종 결과를 내보내기 위한 게이트
	- 모든 Gate는 시그모이드 함수를 사용하고 출력 값이 0이면 아무것도 넘기지 말라, 1이면 모든 것을 넘겨라
- Forget Gate (f) : 과거 정보를 얼마나 잊을지에 대한 단계
![](/assets/img/feature-img/lstm-forget-gate.png)
	- *입력값과 은닉상태를 가중치에 곱한 다음 시그모이드 함수를 통과*시킨 후 0과 1 사이의 값을 이전 Cell State와 곱함
	- 결과값이 1이면 "이전 상태의 모든 정보를 보존해라"가 되고, 0이면 "이전 상태의 모든 정보를 버려라"가 됨

- Input Gate (i) : 현재 정보를 얼마나 기억할 것인지에 대한 단계
![](/assets/img/feature-img/lstm-input-gate.png)
	- *입력값과 Hidden State를 각각 다른 가중치에 곱한 다음 하나는 시그모이드 함수를 통과 시키고, 하나는 tanh함수를 통과시킨 후 두 결과를 곱*한 후 이전 Cell State와 더함

- Output Gate (o) : 다음 State로 내보낼 Output(Hidden state)을 구하는 단계
![](/assets/img/feature-img/lstm-output-gate.png)
	- 입력값과 이전 타임스텝의 Hidden State를 가중치에 곱한 후 활성화 함수(시그모이드)를 통과시키고 이전 Cell State를 tanh 활성화 함수를 통과시킨 값과 곱하여 Hidden State를 만든다


- State Update : 이전 Cell State인 C(t-1)를 업데이트해서 새로운 Cell State인 C(t) 만드는 단계
![](/assets/img/feature-img/lstm-state-update.png)
	- Forget gate를 곱하고
	- Input gate를 더하고
	- 최종 결과를 다음 Cell State로 내보냄


### GRU<sup>Gated Recurrent Unit</sup>
- LSTM을 구성하는 Time-step의 Cell을 조금 더 간소화한 버전
- GRU는 LSTM보다 학습 속도가 빠르다고 알려져있지만, 여러 평가에서 GRU는 LSTM과 비슷한 성능을 보인다고 알려져 있음
- 데이터 양이 적을 때는, 매개 변수의 양이 적은 GRU가 조금 더 낫고 데이터 양이 더 많으면 LSTM이 더 낫다고 알려져 있음
- LSTM구조
![](/assets/img/feature-img/lstm-struct.png)
- GRU구조
![](/assets/img/feature-img/gru-struct.png)
- 차이점
	- GRU는 LSTM과 다르게 Gate가 2개이며, Reset Gate(r)과 Update Gate(z)로 이루어져있다
		- Reset Gate는 이전 상태를 얼마나 반영할지
		- Update Gate는 이전 상태와 현재 상태를 얼마만큼의 비율로 반영할지
	- LSTM에서의 Cell State와 Hidden State가 Hidden State로 통합되었고
	- Update Gate가 LSTM에서의 forget gate, input gate를 제어한다
	- GRU에는 Output Gate가 없다
- Reset Gate
	- 이전 상태의 hidden state와 현재 상태의 x를 받아 sigmoid 처리
	- 이전 hidden state의 값을 얼마나 활용할 것인지에 대한 정보
- Update Gate
	- 이전 상태의 Hidden state와 현재 상태의 x를 받아 sigmoid 처리
	- LSTM의 Forget gate, Input gate와 비슷한 역할을 하며,
	- 이전 정보와 현재 정보를 각각 얼마나 반영할 것인지에 대한 비율을 구하는 것이 핵심이다
		- Update gate의 계산 한 번으로 LSTM의 Forget gate + Input gate의 역할을 대신할 수 있다.
	- 최종 결과는 다음 상태의 Hidden state로 보내짐

### 고급 순환층인 LSTM과 GRU는 SimpleRNN보다 계산이 훨씬 복잡하지만 성능이 뛰어나 순환 신경망에 많이 채택되고 있음
