---
Layout: post
Published: true
title: "확률적 경사 하강법(Stochastic Gradient Descent)"
date: 2021-11-22
excerpt: "확률적 경사 하강법(Stochastic Gradient Descent) 정리"
tags: [Batch Gradient Eescent, Stochastic Gradient Descent, AI, ML, Artificial intelligence, machine learning, megazone, ai center]
author: cindy
---

# 확률적 경사 하강법(Stochastic Gradient Descent)
 - '무작위하게' 혹은 '랜덤하게' 의 기술적 표현이며, 학습 데이터셋에서 무작위로 한개의 샘플 데이터 셋을 추출하고 그 샘플에 대해서만 기울기(최적 해)를 계산하는 최적화 기법(손실함수의  최소 값)

![png](/assets/img/Cindy/stochastic/image8.png)



#### 확률적 경사 하강법 프로세스
![png](/assets/img/Cindy/stochastic/image1.png)


### 장점
1. 매 반복에서 다뤄야 할 데이터 수가 적어 학습 속도가 빠르다.
2. 하나의 샘플 데이터만 사용 하므로 메모리 소모량 낮다.


# 배치 경사 하강법( Batch Gradient Decent,BGD)
 - 전체 샘플 데이터를 사용하여 기울기(최적 해)를 계산

#### 경사하강법과 확률적 경사하강법의 비교
![png](/assets/img/Cindy/stochastic/image2.png)


### 훈련데이터를 모두 다 사용했나요?
## YES (1 에포크 완료)
* 훈련 세트에 샘폴을 모두 채우고 다시 시작
---

## 손실함수(Loss Function)
- 실제값과 예측값의 차이 (loss, cost) 를 수치화해주는 함수
- 오차가 클수록 손실함수의 값이 크고, 오차가 작을수록 손실함수의 값이 작아진다.

** 분류에서의 손실함수
로지스틱 회귀 모델에서는 확률을 출력(시그모이드 함수)

![png](/assets/img/Cindy/stochastic/image7.png)

## 로지스틱 손실 함수
- 다중분류를 위한 손실함수인 엔트로피(cross entropy)손실 함수를 이진 분류 버전으로 만든 함수
(a = 활성화함수 출력한값 , y = 타켓)

#### 로지스틱 손실 함수 해석
-  y(실제값) = 1 (yes) 일 때 or 0 (no) 일 때로 나눈다.
  
![png](/assets/img/Cindy/stochastic/image4.png)

![png](/assets/img/Cindy/stochastic/image5.png)

1.  y 가 1일 경우, 예측한 값이 1이면 cost = 0 으로 최저 
->  0에 가까울 수록 cost 올라감
2. y 가 0일 경우, 예측한 값이 0이면 cost = 0 으로 최저
->  1에 가까울 수록 cost 올라감

### 로지스틱 손실 함수 미분
![png](/assets/img/Cindy/stochastic/image6.png)
---
### 에포크와 과대/과소적합
* 훈련데이터셋에 포함된 모든 데이터들이 한번씩 모델을 통과한 횟수로, 모든 학습 데이터셋을 학습하는 횟수를 의미
* 확률적 경사하강법을 사용한 모델은 에포크 횟수에 따라 과소적합이나 과대적합이 될 수 있다.

