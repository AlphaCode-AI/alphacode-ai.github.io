---
layout: post
published: true
title: "주성분 분석"
Date: 2021-06-15
tags: [study, 스터디]
author: debbie
---

# 주성분 분석<sup>Principal Component Analysis</sup>

- 데이터에 있는 분산<sup>데이터가 널리 퍼져있는 정도</sup> 이 큰 방향을 찾는 것
- 고차원의 데이터를 저차원으로 축소시키는 대표적인 차원 축소 알고리즘 중 하나
- 데이터들을 정사영 시켜 차원을 낮춘다면 어떤 벡터에 데이터를 정사영 시켜야 원래 데이터 구조를 제일 잘 유지할 수 있을까
- 기존의 데이터 포인트들과 낮춘 차원의 데이터 구조와의 차가 가장 작게 하는 방식으로 진행
- 변수가 너무 많아 이들 중 중요하다고 판단되는 변수들 몇 개만 뽑아 모델링을 하려고 할 때 주로 PCA를 사용한다.


## PCA가 필요한 이유

### 1) 차원의 저주

- 데이터셋의 feature가 많아지면 차원 또한 증가

- 데이터의 차원이 증가할 수록 데이터 공간의 부피가 기하급수적으로 증가 -> 데이터의 밀도는 차원이 증가할 수록 희소해짐

- 데이터의 차원이 증가할 수록 데이터 포인트 간의 거리도 증가 -> 모델이 복잡해져 오버피팅 위험 커짐

  

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=http%3A%2F%2Fcfile1.uf.tistory.com%2Fimage%2F99FF9F335B8A484A31820B">

### 2) 다중 공산성 문제

 - 회귀분석의 전제조건. 독립변수간의 상관관계가 높으면 안됨
 - 서로 의존성이 높은 feature들을 함께 학습시키면 오버피팅되는 경우 생김





## PCA 과정

<img src="https://blog.kakaocdn.net/dn/MBCIR/btqxCOf0KX2/iAb073nczwkPb9gbqdSxj0/img.png" width="600">







### 1) 기존의 데이터 축소

 - x1축으로 축소를 할 수도 x2축으로 축소를 할 수도 있음
 - 이 과정에서 데이터가 겹치는 문제 발생 -> 정보 유실

<img src="https://blog.kakaocdn.net/dn/dzWKV8/btqxDvAfiig/1HyNGO6H6QGi1IW6dsrMw0/img.png" width="400" style="float:left">



<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fcd4ZiP%2FbtqxyhRvkho%2FhkzBBrhEsWEkXGTLtRqKlk%2Fimg.png" width="400" style="float:left"><br><br><br><br>















### 2) 새로운 축 찾기

 - x1과 x2축이 아닌 새로운 축을 찾아야함
 - 축은 각 점들이 퍼져있는 정도인 분산이 가장 크게 되도록 잡아야함
 - 이 축의 벡터를 주성분(principal component) 이라고 한다.

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fcw4jfN%2FbtqxC6HrkI1%2FkK1EGPIHt8sKjzFgocx31K%2Fimg.png" width="600">







## 원본 데이터를 어떤 벡터에 내적하는 것이 최적의 결과를 내주는가







## Example

billboard hot100 chart audio-feature (feature 11개)



```python
trackdf
```



![test](/assets/img/yunju/pca/screenshot01.png)







### 11개 feature를 주성분분석을 활용하여 2개로 줄여서 현재 데이터 표현하기


```python
featurelist = list(trackdf.columns)[1:]
tracktrain = trackdf[featurelist]
```


```python
tracktrain
```



![test](/assets/img/yunju/pca/screenshot02.png)









tracktrain 정규화


```python
Scaler = StandardScaler()
tracktrain = Scaler.fit_transform(tracktrain)
tracktrain = pd.DataFrame(tracktrain,columns=featurelist)
```


```python
tracktrain
```


![test](/assets/img/yunju/pca/screenshot03.png)







PCA -> 2차원 데이터로 만들기


```python
pca = PCA(n_components=2)
principal = pca.fit(tracktrain)
principaltransform = pca.fit_transform(tracktrain)
pc = principal.components_
principalDF = pd.DataFrame(data = principaltransform, columns = ['principalcomponent01','principalcomponent02'])
```



주성분벡터


```python
pc
```


    array([[-0.30323406, -0.5291131 , -0.06083497, -0.45635198,  0.10039601,
            -0.01568293,  0.49884875, -0.13077555, -0.0150439 , -0.36232136,
             0.09137661],
           [ 0.44035325, -0.22264418,  0.33311453, -0.33782897, -0.39143413,
             0.41255903,  0.0387862 ,  0.30700042, -0.27256214,  0.17523132,
             0.08514396]])




```python
principalDF
```


![test](/assets/img/yunju/pca/screenshot04.png)








## 다른 알고리즘 활용하기

### 선형 회귀 분석

- 특성과 타겟사이의 관계를 가장 잘 나타내는 선형방정식 찾기


```python
line_filter = LinearRegression()
p_component01 = np.array(principalDF["principalcomponent01"])
p_component02 = np.array(principalDF["principalcomponent02"])
p_component01 = p_component01.reshape(-1,1)
line_filter.fit(p_component01, p_component02)
print("z = {}*x + {}".format(line_filter.coef_[0], line_filter.intercept_))
```

    z = 5.296358786892319e-16*x + -4.440892098500625e-17



```python
plt.scatter(principalDF["principalcomponent01"],principalDF["principalcomponent02"])
plt.plot([-3,5],[(-3)*line_filter.coef_+line_filter.intercept_, 5*line_filter.coef_+line_filter.intercept_],color="r")

plt.ylim(-5,5)
plt.xlabel("principalcomponent01")
plt.ylabel("principalcomponent02")
plt.show()
```


![test](/assets/img/yunju/pca/screenshot05.png)

